<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta
      name="description"
      content="OfflineAI case study: a privacy-first, offline iOS app running small language models fully on-device."
    />

    <title>OfflineAI - Case Study</title>

    <link rel="icon" href="../../assets/favicon.svg" type="image/svg+xml" />
    <link rel="stylesheet" href="../../styles.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fraunces:opsz,wght@9..144,500;9..144,700&family=Space+Grotesk:wght@400;600;700&display=swap"
      rel="stylesheet"
    />

    <script src="../../script.js" defer></script>
  </head>
  <body>
    <a class="skip-link" href="#content">Skip to content</a>

    <header class="site-header">
      <div class="container header-inner">
        <a class="brand" href="../../" aria-label="Home">
          <span class="brand-mark" aria-hidden="true">I</span>
          <span class="brand-text">Imran Tanbir</span>
        </a>

        <nav class="site-nav" aria-label="Primary">
          <a href="../../#about">About</a>
          <a href="../../#skills">Skills</a>
          <a href="../../#projects">Projects</a>
          <a href="../../#experience">Experience</a>
          <a href="../../#contact" class="nav-cta">Contact</a>
        </nav>
      </div>
    </header>

    <main id="content" class="page">
      <section class="section">
        <div class="container">
          <p class="crumb" data-reveal>
            <a href="../../#projects">Projects</a>
            <span class="sep" aria-hidden="true">/</span>
            <span>OfflineAI</span>
          </p>

          <div class="project-hero" data-reveal>
            <div class="project-hero-top">
              <img
                class="project-icon"
                src="../../assets/mobile-app.svg"
                width="64"
                height="64"
                alt=""
                aria-hidden="true"
              />
              <div>
                <h1 class="project-title">OfflineAI</h1>
                <p class="project-subtitle">
                  A production-grade offline-first iOS app running small language
                  models completely on-device, with privacy-first design and
                  intelligent resource management.
                </p>
              </div>
            </div>

            <div class="project-actions">
              <a class="btn btn-primary" href="../../#contact">Work with me</a>
              <a
                class="btn btn-ghost"
                href="https://github.com/imranow/OfflineAI"
                target="_blank"
                rel="noreferrer"
                >Repo</a
              >
              <a class="btn btn-ghost" href="#" aria-disabled="true">Live</a>
            </div>
          </div>

          <div class="prose" data-reveal>
            <h2>What it is</h2>
            <p>
              OfflineAI is a privacy-first iOS app that runs small language
              models completely offline. There are no cloud dependencies and no
              API costs, while still supporting a usable, fast experience on
              real devices.
            </p>

            <h2>Features</h2>
            <ul>
              <li>
                <strong>100% offline:</strong> all inference runs locally on
                device.
              </li>
              <li><strong>Zero API costs:</strong> no cloud dependencies.</li>
              <li>
                <strong>Complete privacy:</strong> AES-256 encryption and no
                telemetry.
              </li>
              <li>
                <strong>Intelligent resource management:</strong> dynamic
                quantisation (4-bit/8-bit), LRU model caching with automatic
                unloading, memory pressure monitoring, and battery-aware
                processing.
              </li>
              <li>
                <strong>Context management:</strong> semantic chunking with
                embedding-based relevance.
              </li>
              <li>
                <strong>Offline-first sync (optional):</strong> optional
                encrypted cloud sync.
              </li>
            </ul>

            <h2>Architecture</h2>
            <p>
              The app is split into a few core components that keep inference
              reliable under real-world constraints (memory, battery, and device
              performance variability).
            </p>
            <div class="callouts">
              <div class="callout">
                <h3>Model management</h3>
                <p>
                  Lazy loading with an LRU cache, automatic unloading on memory
                  pressure, preloading during idle + charging, and support for
                  multiple quantisation levels.
                </p>
              </div>
              <div class="callout">
                <h3>Device profiling</h3>
                <p>
                  Memory detection/monitoring, battery state tracking, automatic
                  model selection, and performance benchmarking.
                </p>
              </div>
              <div class="callout">
                <h3>Inference engine</h3>
                <p>
                  Streaming token generation, batch processing for efficiency,
                  and embedding generation.
                </p>
              </div>
              <div class="callout">
                <h3>Data layer</h3>
                <p>
                  SwiftData local persistence with AES-256-GCM encryption and
                  per-conversation encryption keys.
                </p>
              </div>
            </div>

            <h2>Supported models</h2>
            <h3>Phi-3-Mini (recommended)</h3>
            <ul>
              <li>Parameters: 3.8B</li>
              <li>Context: 4096 tokens</li>
              <li>Quantisations: Q4_0 (~2.1 GB), Q8_0 (~3.9 GB)</li>
              <li>Best for: modern devices (iPhone 12+)</li>
            </ul>

            <h3>TinyLlama (fallback)</h3>
            <ul>
              <li>Parameters: 1.1B</li>
              <li>Context: 2048 tokens</li>
              <li>Quantisation: Q4_0 (~0.6 GB)</li>
              <li>Best for: older devices or low memory</li>
            </ul>

            <h2>What I would demo</h2>
            <ol>
              <li>On-device model selection (Phi-3 vs TinyLlama)</li>
              <li>Quantisation switching (4-bit/8-bit) based on device profile</li>
              <li>Memory pressure handling and LRU cache eviction</li>
              <li>Encrypted local storage for conversations</li>
            </ol>

            <h2>Next</h2>
            <p class="next-links">
              <a class="btn btn-small btn-ghost" href="../fraud-ml/"
                >Next: Fraud ML</a
              >
            </p>
          </div>
        </div>
      </section>
    </main>

    <footer class="site-footer">
      <div class="container footer-inner">
        <p class="footer-left">
          <span class="footer-mark" aria-hidden="true"></span>
          <span>Imran Tanbir</span>
        </p>
        <p class="footer-right">
          <a href="../../">Home</a>
          <span class="sep" aria-hidden="true">/</span>
          <a href="../../#projects">Projects</a>
          <span class="sep" aria-hidden="true">/</span>
          <a href="../../#contact">Contact</a>
        </p>
      </div>
    </footer>
  </body>
</html>
